üèõÔ∏è **architect-agent**: Acceptance Criteria para T019 ‚Äî Agent Factory

# Acceptance Criteria ‚Äî 19-agent-factory

## 1. Consolidated Definition

Crear un **Agent Factory** dentro del sidecar backend (Fastify :3000) que:
- Provea una abstracci√≥n de **LLM Client** agn√≥stica al provider
- Soporte **todos los providers registrados en Settings** (no solo los 3 actuales)
- Incluya **streaming** (SSE) desde el inicio
- Ofrezca capacidades completas de agente: **function calling, tool use, system prompts** (no solo chat)
- Implemente **binding role ‚Üí modelo** para que cada agente del sistema tenga un modelo LLM asignado
- Viva en el **sidecar process** (aislado del Extension Host)

## 2. Answers to Clarification Questions

| # | Question (formulated by architect) | Answer (from developer) |
|---|-----------------------------------|------------------------|
| 1 | ¬øSolo 3 providers o extensibilidad para custom? | Debe soportar todos los providers que se registren en Settings. Por OAuth hay 2 fijos, por API key puede haber muchos. Extensible. |
| 2 | ¬øLLM client en sidecar o Extension Host? | En el sidecar backend (Fastify). Analizar la mejor opci√≥n. |
| 3 | ¬øStreaming desde el inicio o despu√©s? | S√≠, streaming desde el inicio. |
| 4 | ¬øRole-Model binding en esta tarea? | S√≠, asignaci√≥n agente ‚Üí LLM incluida en esta tarea. |
| 5 | ¬øSolo chat completions o m√°s? | Todo lo que necesita un agente para ser m√°s que un chat: function calling, tool use, etc. |

---

## 3. Verifiable Acceptance Criteria

1. Scope:
   - LLM client abstraction layer + factory + role-model binding dentro del sidecar backend
   - Extensible a cualquier provider registrado en Settings

2. Inputs / Data:
   - `LLMModelConfig` desde Settings (provider, authType, apiKey, modelName, maxTokens, temperature)
   - Mensajes de chat (`ChatMessage[]`) con roles (system, user, assistant)
   - Definici√≥n de tools/functions para function calling
   - Role ‚Üí Model mapping configuration

3. Outputs / Expected Result:
   - `LLMClient` interface con: `chat()`, `chatStream()`, soporte para tool use
   - `LLMFactory` que crea el client correcto por provider
   - Endpoint `/chat` (command) y `/chat/stream` (SSE) en el sidecar
   - Configuraci√≥n persistente de role ‚Üí model binding
   - Streaming funcional para todos los providers

4. Constraints:
   - El LLM client vive en el sidecar (NO en Extension Host)
   - Extensible: no hardcoded a 3 providers
   - Seguir constituci√≥n clean_code y modular_architecture
   - El Extension Host pasa tokens OAuth al sidecar (no acceso directo)

5. Acceptance Criterion (Done):
   - [ ] `npm run compile` exitoso
   - [ ] E2E tests pasan sin regresi√≥n
   - [ ] Al menos 1 provider funcional end-to-end (chat + stream)
   - [ ] Role ‚Üí Model binding configurable
   - [ ] Function/tool calling interface definida
   - [ ] Arquitectura extensible para nuevos providers

---

## Approval (Gate 0)

```yaml
approval:
  developer:
    decision: SI
    date: "2026-02-18T17:37:14+01:00"
    comments: null
```

---

## Validation History (Phase 0)
```yaml
history:
  - phase: "phase-0-acceptance-criteria"
    action: "created"
    validated_by: "architect-agent"
    timestamp: "2026-02-18T17:35:43+01:00"
    notes: "Acceptance criteria defined from 5 developer answers"
  - phase: "phase-0-acceptance-criteria"
    action: "approved"
    validated_by: "developer"
    timestamp: "2026-02-18T17:37:14+01:00"
    notes: "Developer approved ‚Äî SI"
```
